==================[Syllabus Start]==================
CS229: Machine Learning - The Summer Edition!
Course Description
   This is the summer edition of CS229 Machine Learning that was offered over 2019 and 2020. CS229
provides a broad introduction to statistical machine learning (at an intermediate / advanced level) and covers supervised
learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines);
unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs,
practical ); and reinforcement learning among other topics. 
The structure of the summer offering enables coverage of
additional topics, places stronger emphasis on the mathematical and visual intuitions, and goes deeper into the
details of various topics
.
Full playlist (YouTube)
Stanford CS229: Machine Learning | Summer 2019 | Lecture 1 - Introd
…
Syllabus and Course Schedule
Event
Date
Description
Materials and Assignments
Introduction and Pre-requisties review
 (3 lectures)
Lecture 1
[
YouTube
]
6/24
Introduction and Logistics
Review of Linear Algebra
Class Notes
Introduction [
pptx
]
Linear Algebra (section 1-3) [
pdf
]
Lecture 2
[
YouTube
]
6/26
Review of Matrix Calculus
Review of Probability
Class Notes
Linear Algebra (section 4) [
pdf
]
Probability Theory [
pdf
]
Probability Theory Slides [
pdf
]
Lecture 3
[
YouTube
]
6/28
Review of Probability and
Statistics
Setting of Supervised Learning
Class Notes
Supervised Learning [
pdf
]
Probability Theory [
pdf
]
Supervised Learning
 (8 lectures)
Lecture 4
[
YouTube
]
7/1
Linear Regression
[Stochastic] Gradient Descent
([S]GD)
Normal Equations
Probabilistic Interpretation
Maximum Likelihood Estimation
(MLE)
Class Notes
Supervised Learning (section 1-3) [
pdf
]
Lecture 5
[
YouTube
]
7/3
Perceptron
Logistic Regression
Newton's Method
Class Notes
Supervised Learning (section 5-7) [
pdf
]
Lecture 6
[
YouTube
]
7/5
Exponential Family
Generalized Linear Models (GLM)
Class Notes
Supervised Learning (section 8-9) [
pdf
]
Lecture 7
[
YouTube
]
7/8
Gaussian Discriminant Analysis
(GDA)
Naive Bayes
Laplace Smoothing
Class Notes
Generative Algorithms [
pdf
]
Lecture 8
[
YouTube
]
7/10
Kernel Methods
Support Vector Machine
Class Notes
Kernel Methods and SVM [
pdf
]
Lecture 9
[
YouTube
]
7/12
Bayesian Methods
Parametric (Bayesian Linear
Regression)
Non-parametric (Gaussian
process)
Class Notes
Gaussian Processes [
pdf
]
Optional
The Multivariate Gaussian Distribution
[
pdf
]
More on Gaussian Distribution [
pdf
]
Lecture 10
[
YouTube
]
7/15
Neural Networks and Deep
Learning
Class Notes
Deep Learning (skip Sec 3.3) [
pdf
]
Optional
Backpropagation [
pdf
]
Lecture 11
[
YouTube
]
7/17
Deep Learning (contd)
Theory
 (2 lectures)
Lecture 12
[
YouTube
]
7/19
Bias and Variance
Regularization, Bayesian
Interpretation
Model Selection
Class Notes
Regularization and Model Selection [
pdf
]
Lecture 13
[
YouTube
]
7/22
Bias-Variance tradeoff (wrap-up)
Empirical Risk Minimization
Uniform Convergence
Class Notes
Bias Variance Analysis [
pdf
]
Statistical Learning Theory [
pdf
]
Reinforcement Learning
 (2 lectures)
Lecture 14
[
YouTube
]
7/24
Reinforcement Learning (RL)
Markov Decision Processes (MDP)
Value and Policy Iterations
Class Notes
Reinforcement Learning and Control (Sec
1-2) [
pdf
]
Lecture 15
[
YouTube
]
7/26
RL (wrap-up)
Learning MDP model
Continuous States
Class Notes
Reinforcement Learning and Control (Sec
3-4) [
pdf
]
Unsupervised Learning
 (3 lectures)
Lecture 16
[
YouTube
]
7/29
Unsupervised Learning
K-means clustering
Mixture of Gaussians (GMM)
Expectation Maximization (EM)
Class Notes
K-means [
pdf
]
Mixture of Gaussians [
pdf
]
Expectation Maximization (Sec 1-2, skip
2.1) [
pdf
]
Lecture 17
[
YouTube
]
7/31
EM (wrap-up)
Factor Analysis
Class Notes
Expectation Maximization (Sec 3) [
pdf
]
Factor Analysis [
pdf
]
Lecture 18
[
YouTube
]
8/2
Factor Analysis (wrap-up)
Principal Components Analysis
(PCA)
Independent Components
Analysis (ICA)
Class Notes
Principal Components Analysis [
pdf
]
Independent Components Analysis [
pdf
]
Miscellaneous Topics
 (3 lectures)
Lecture 19
8/5
Maximum Entropy and
Exponential Family
KL-Divergence
Calibration and Proper Scoring
Rules
Class Notes
Maximum Entropy [
pdf
]
Lecture 20
8/7
Variational Inference
EM Variants
Variational Autoencoder
Class Notes
VAE (Sec 4) [
pdf
]
Lecture 21
8/9
Evaluation Metrics
Class Notes
Evaluation Metrics [
pptx
]
Recap and wrap-up
 (2 lectures)
Lecture 22
8/12
Practical advice and tips
Review for Finals
Class Notes
Lecture 23
8/14
Review for Finals
Class Notes
Final
8/16
Other Resources
1
. 
Advice on applying machine learning: Slides from Andrew's lecture on getting machine learning algorithms to work
in practice can be found 
here
.
2
. 
Previous projects: A list of last year's final projects can be found 
here
.
3
. 
Data: Here is the 
UCI Machine learning repository
, which contains a large collection of standard datasets for testing
learning algorithms. If you want to see examples of recent work in machine learning, start by taking a look at the
conferences 
NeurIPS
 (all old NeurIPS papers are online) and ICML. Some other related conferences include UAI,
AAAI, IJCAI.
4
. 
Viewing PostScript and PDF files: Depending on the computer you are using, you may be able to download a
PostScript
 viewer or 
PDF viewer
 for it if you don't already have one.
5
. 
Machine learning study guides tailored to CS 229
 by Afshine Amidi and Shervine Amidi.

===================[Syllabus End]===================
Please examine the attached course syllabus carefully and provide detailed answers to the research questions (RQ) listed below. Each question focuses on specific aspects of "computing systems" tailored for AI/ML scalability. We are looking for specific issues and topics related to compilers, runtime systems, hardware acceleration, code optimization, programming model for AI/ML covered in the syllabus. Programming with Python or jupyter does not count as computing system topics.

RQ 1. Course Content and Frequency:
1.1 How frequently are topics explicitly related to "computing system" specialized for ML/AI discussed in the course? 
The topics are 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in likert scale: 
Frequent (4): At least one dedicated lecture discussed the topics.
Intermittent (3): The topics are discussed occasionally. 
Infrequent (2): The topics are rarely mentioned.
Never mentioned (1): The topics are never mentioned.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 2. Definition and Understanding:
2.1 How are the impacts of "computing systems" on AI/ML explicitly defined and explained in undergraduate curricula? 
The definition and explanation should include concepts of 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Adequate (3): Provide detailed definition and explanation.
Inadequate (2): Many of the topics missed significant discussion in lectures or in assignments.
Undefined (1): The topics are mostly undefined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
2.2 Do courses provide a comprehensive and explicit definition of impacts of "computing systems" on AI/ML?
The definition and explanation should include concepts such as 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer by providing the list of above topics (1 to 9) discussed in the course. Make it short and direct. Limit in 100 words. Do not include topics unrelated to "computing systems" like general ML/AI algorithms.

RQ 3. Requirement Specification:
3.1 How are computational performance and capability requirements for hardware and software systems running scalable AI/ML, explicitly specified and discussed in undergraduate courses?
Topics include 1) Computational Power (CPU, GPU, TPU, Edge AI chips), Memory and Storage, Network for scalable (parallel and distributed) model training, inference; 2) Distributed Computing Frameworks such as TensorFlow's Distributed Strategy, PyTorch's Distributed Data Parallel (DDP), and Horovod 3)  Optimization Techniques such as Efficient Algorithm, Quantization, Prunning 4) Programming Models and Abstractions such as High-Level Libraries (Tensorflow, PyTorch, Keras)
Answer in Likert scale: 
Quantitatively (3): The lectures or assignments provide numerical values for computational performance and capability requirements such as latency, throughput, resource utilization etc.
Qualitatively (2): The lectures used descriptive terms.
No guidelines (1): The Lecture provide no guidelines.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
3.2 How did the discussion of “computing system” requirements rank against the discussion of general AI/ML topics?
Answer in Likert scale: 
Equally discussed with other AI/ML topics (3)
“computing system” requirements is a sub topic (2) 
“computing system” requirements were never discussed (1) 
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 4. Influence and Importance:
4.1 How is the importance of various “computing system” factors of designing and maintaining scalable AI/ML emphasized in the course?
The factors are 1) scalable (parallel and distributed) model training and inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Holistic (2): The course took into account of many of the above factors.
System (1): The course viewed the factors as low level system issue, relegating responsibility to correct choice of hardware, programming model and AI/ML framework.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.

RQ 5. Case Studies and Real-World Applications:
5.1 Are real-world case studies involving hardware and software systems for AI/ML, with a focus on scalable model training, inference, and serving explicitly included in the curriculum?
Answer in Likert scale: 
Major (2): Computational performance and capability of the underlying system was the major concerns of the case studies.
Minor (1): Computational performance and capability of the underlying system was not a major concern of the case studies.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 6. Awareness and Integration of AI-Specific Engineering Practices:
6.1 Do the courses discuss contributions and best practices from AI/ML system engineering communities, specifically in areas such as compilers, runtime systems, hardware acceleration, and code optimization?
Answer in Likert scale: 
Adequate (3): The courses thoroughly cover contributions from AI/ML system engineering communities and best practices in detail by depicting from state of art.
Inadequate (2): The courses mention the topic but do not cover it in sufficient depth or detail.
Undefined (1): The coverage of this topic in the courses is unclear or not well-defined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 7. Projects and Practical Implementation:
7.1 To what extent do the assignments in the course provide hands-on experience with designing, building, and maintaining both scalable hardware and software systems for AI/ML, specifically focusing on compiler optimization, optimizing runtime systems, hardware acceleration, or code optimization for AI/ML?
Answer in Likert scale: 
Adequate (3): The assignments thoroughly cover these areas and provide extensive hands-on experience.
Inadequate (2): The assignments cover these areas minimally and do not provide sufficient hands-on experience.
None (1): The assignments do not cover these areas or provide relevant hands-on experience.
Could not be evaluated (0): Insufficient information or exposure to the assignments on the syllabus to provide an evaluation.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
