==================[Syllabus Start]==================
9/29/2018 Stat 231: Pattern Recognition and Machine Learning
http://www .stat.ucla.edu/~sczhu/Courses/UCLA/Stat_231/Stat_231.html 1/2Stat 231 / CS 276A
 Pattern Recognition and Machine Learning
Fall 2018, MW 3:30-4:45 PM, Franz Hall 1260
 www .stat.ucla.edu/~sczhu/Courses/UCLA/Stat_231/Stat_231.html
Course Description
This course introduces fundamental concepts, theories, and algorithms for pattern recognition and machine learning, 
which are used in computer vision, speech recognition, data mining, statistics, information retrieval, and bioinformatics. 
Topics include: Bayesian decision theory , parametric and non-parametric learning, data clustering, component analysis, 
boosting techniques, support vector machine, and deep learning with neural networks.
Prerequisites
You don't have to take exactly these courses as long as you know the materials.
Math 33A Linear Algebra and Its Applications, Matrix Analysis.
Stat 100B Intro to Mathematical Statistics.
CS 180 Intro to Algorithms and Complexity .
Programming skills in Matlab or Python.
Textbook
Textbook is not mandatory if you can understand the lecture notes and handouts.
R. Duda, et al., Pattern Classification , John W iley & Sons, 2001. [Good for CS students]
T. Hastie, et al., The Elements of Statistical Learning , Spinger , 2009. [Good for Stat students]
C. Bishop, Pattern Recognition and Machine Learning , Springer , 2006. [with advanced materials]
Instructors
Prof. Song-Chun Zhu , sczhu@stat.ucla.edu, Of fice: Boelter Hall 9404 
 Office Hours: T uesday 2-3pm
TAs for ST AT 231A: Ruiqi Gao  ruiqigao@ucla.edu, Erik Nijkamp , enijkamp@ucla.edu. 
 TAs for CS276A: Luyao Y uan, yuanluxu@cs.ucla.edu 
 Office Hours: Session 1: T uesday 6-7pm and Session 2: 7-8pm for project overview and assistant. 
 Individual questions will be answered by the readers through CCLE online.
Grading Plan
Five pr ojects:
0. Exer cise on Classification using Deep Learning and CNN 
Experience the training and testing of pr oblems in machine learning
1. Face modeling by AAM Model, Auto-encoder and FLD 
How many bits do you need to r epresent a face? Or telling apart male fr om female faces?
2. Face detection by AdaBoost and RealBoost 
How many featur es do you need to detect a face in a cr owd?
3. Object detection by Fast-RCNN 
How to detect the class of an object and its location?
4. Face social attributes and sentiment analysis by SVM  
How do we measur e the social dimensions of faces in political elections and social network? 
10%
15%
15%
10%
10%
9/29/2018 Stat 231: Pattern Recognition and Machine Learning
http://www .stat.ucla.edu/~sczhu/Courses/UCLA/Stat_231/Stat_231.html 2/2Final Exam: Friday , December 14, 11:30 AM - 2:30 PM Franz Hall 1260 40%
Grading Policy
You are encouraged to work and discuss in a group, but each person must finish his/her own project. Submit your report
of the project, and your code through the CCLE website .
You have a total of 5 late days (not including weekends) for the entire class (5 projects) to cover your various reasons,
but after using the four late days, no credit will be given for late homework/project.
Schedule
Tentative Schedule for 2018 (Course materials will be posted on the CCLE site.)
Lecture  Topics Handouts
1  Introduction to Pattern Recognition 
[Problems, applications, examples] 
2  Bayesian Decision Theory I 
[Bayes rule, discriminant functions] 
3  Bayesian Decision Theory II 
 [loss functions and Bayesian error analysis] 
4  Component Analysis and Dimension Reduction I :
 [PCA, face modeling by Active Appearance Model (AAM), Auto-encoder] 
5  Component Analysis and Dimension Reduction II
 [Fisher Linear Discriminant ] [Multi-dimensional scaling (MDS)] 
6  Component Analysis and Dimension Reduction III
 [Local Linear Embedding (LLE), Intrinsic dimension] 
7  Boosting T echniques I
 [perceptron, backpropagation and Adaboost] 
8  Boosting T echniques II
 [RealBoost and Example on face detection] 
9  Boosting T echniques III
 [analysis, logit boost, cascade and decision policy] 
10  Boosting T echniques III
 [analysis, logit boost, cascade and decision policy] 
11  Non-metric method I
 [Decision tree and random forrest] 
12  Non-metric method II
 [Syntactic pattern recognition and example on human parsing] 
13  Support vector machine I
 [Kernel-induced feature space] 
14  Support vector machine II
 [Support vector classifier] 
15  Support vector machine III
 [Loss functions, Latent SVM] 
16  Parametric Learning
 [ Maximum Likelihood Estimation ] [ Suf ficient Statistics and Maximum entropy] 
17  Non-parametric Learning I
 [ Parzen window and K-nn classifer] 
18  Non-parametric Learning II
 [K-nn classifer and Error analysis] 
19  Deep Learning I
20  Deep Learning II

===================[Syllabus End]===================
Please examine the attached course syllabus carefully and provide detailed answers to the research questions (RQ) listed below. Each question focuses on specific aspects of "computing systems" tailored for AI/ML scalability. We are looking for specific issues and topics related to compilers, runtime systems, hardware acceleration, code optimization, programming model for AI/ML covered in the syllabus. Programming with Python or jupyter does not count as computing system topics.

RQ 1. Course Content and Frequency:
1.1 How frequently are topics explicitly related to "computing system" specialized for ML/AI discussed in the course? 
The topics are 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in likert scale: 
Frequent (4): At least one dedicated lecture discussed the topics.
Intermittent (3): The topics are discussed occasionally. 
Infrequent (2): The topics are rarely mentioned.
Never mentioned (1): The topics are never mentioned.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 2. Definition and Understanding:
2.1 How are the impacts of "computing systems" on AI/ML explicitly defined and explained in undergraduate curricula? 
The definition and explanation should include concepts of 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Adequate (3): Provide detailed definition and explanation.
Inadequate (2): Many of the topics missed significant discussion in lectures or in assignments.
Undefined (1): The topics are mostly undefined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
2.2 Do courses provide a comprehensive and explicit definition of impacts of "computing systems" on AI/ML?
The definition and explanation should include concepts such as 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer by providing the list of above topics (1 to 9) discussed in the course. Make it short and direct. Limit in 100 words. Do not include topics unrelated to "computing systems" like general ML/AI algorithms.

RQ 3. Requirement Specification:
3.1 How are computational performance and capability requirements for hardware and software systems running scalable AI/ML, explicitly specified and discussed in undergraduate courses?
Topics include 1) Computational Power (CPU, GPU, TPU, Edge AI chips), Memory and Storage, Network for scalable (parallel and distributed) model training, inference; 2) Distributed Computing Frameworks such as TensorFlow's Distributed Strategy, PyTorch's Distributed Data Parallel (DDP), and Horovod 3)  Optimization Techniques such as Efficient Algorithm, Quantization, Prunning 4) Programming Models and Abstractions such as High-Level Libraries (Tensorflow, PyTorch, Keras)
Answer in Likert scale: 
Quantitatively (3): The lectures or assignments provide numerical values for computational performance and capability requirements such as latency, throughput, resource utilization etc.
Qualitatively (2): The lectures used descriptive terms.
No guidelines (1): The Lecture provide no guidelines.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
3.2 How did the discussion of “computing system” requirements rank against the discussion of general AI/ML topics?
Answer in Likert scale: 
Equally discussed with other AI/ML topics (3)
“computing system” requirements is a sub topic (2) 
“computing system” requirements were never discussed (1) 
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 4. Influence and Importance:
4.1 How is the importance of various “computing system” factors of designing and maintaining scalable AI/ML emphasized in the course?
The factors are 1) scalable (parallel and distributed) model training and inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Holistic (2): The course took into account of many of the above factors.
System (1): The course viewed the factors as low level system issue, relegating responsibility to correct choice of hardware, programming model and AI/ML framework.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.

RQ 5. Case Studies and Real-World Applications:
5.1 Are real-world case studies involving hardware and software systems for AI/ML, with a focus on scalable model training, inference, and serving explicitly included in the curriculum?
Answer in Likert scale: 
Major (2): Computational performance and capability of the underlying system was the major concerns of the case studies.
Minor (1): Computational performance and capability of the underlying system was not a major concern of the case studies.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 6. Awareness and Integration of AI-Specific Engineering Practices:
6.1 Do the courses discuss contributions and best practices from AI/ML system engineering communities, specifically in areas such as compilers, runtime systems, hardware acceleration, and code optimization?
Answer in Likert scale: 
Adequate (3): The courses thoroughly cover contributions from AI/ML system engineering communities and best practices in detail by depicting from state of art.
Inadequate (2): The courses mention the topic but do not cover it in sufficient depth or detail.
Undefined (1): The coverage of this topic in the courses is unclear or not well-defined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 7. Projects and Practical Implementation:
7.1 To what extent do the assignments in the course provide hands-on experience with designing, building, and maintaining both scalable hardware and software systems for AI/ML, specifically focusing on compiler optimization, optimizing runtime systems, hardware acceleration, or code optimization for AI/ML?
Answer in Likert scale: 
Adequate (3): The assignments thoroughly cover these areas and provide extensive hands-on experience.
Inadequate (2): The assignments cover these areas minimally and do not provide sufficient hands-on experience.
None (1): The assignments do not cover these areas or provide relevant hands-on experience.
Could not be evaluated (0): Insufficient information or exposure to the assignments on the syllabus to provide an evaluation.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
