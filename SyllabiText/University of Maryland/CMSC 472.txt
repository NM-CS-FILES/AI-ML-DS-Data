==================[Syllabus Start]==================
Introduction to 
Deep Learning
CMSC472 · Spring 2021 · 
Unive
rsity of Maryland
Description
This course is an elementary introduction to a machine learning technique called deep learning, as well as 
its applications to a variety of
domains. Along the way, the course also provides an intuitive introduction 
to machine learning such as simple models, learning paradigms,
optimization, overfitting, importance of 
data, training caveats, etc. The assignments explore key concepts and simple applications, and the
final 
project allows an in-depth exploration of a particular application area. By the end of the course, you will 
have an overview on the deep
learning landscape and its applications. You will also have a working knowledge 
of several types of neural networks, be able to implement
and train them, and have a basic understanding of 
their inner workings.
Logistics
Where & when
Online 
Tuesday, Thursday 2:00pm - 3:15pm
Instructor
Abhinav Shrivastava
 
abhinav@cs.umd.edu
Office hours: Tuesdays 4:00-5:00pm (or by email) (see Piazza for details).
Teaching Assistant
Vinoj Jayasundara
 
vinoj@umd.edu
Office hours: Wednesday, 4:30-5:00pm
Matthew Gwilliam
 
mgwillia@umd.edu
Office hours: Monday, 1:30-2:00pm
Soumik Mukhopadhyay
 
smukhopa@umd.edu
Office hours: Friday, 10:00-11:00am (every other week)
Quick Links
ELMS-Canvas
 
Piazza
 
Web Accessibility
 
Schedule
Schedule
Date
Date
Topic
Topic
Slides
Slides
Notes &
Notes &
Assignments
Assignments
January 26
Course Introduction
Motivation 
Goals, Syllabus, Assignments
Policies
slides
Machine Learning Basics - I
January 28
Introduction to Statistical Learning
Simple models 
Paradigms of learning
slides
Neural Networks Basics - I
February 2
library_books
Introduction to Neural Networks
Terminology 
Simple Neural Networks 
Non-linearities 
slides
Assignment 1 out
(Gradescope)
February 4
Problem setup: labels and losses
Types of problems and labels 
Loss functions
slides
Machine Learning Basics - II
February 9
library_books
February 11
Optimization basics
Loss function derivatives, minimas
Gradient descent, Stochastic gradient descent
slides
Assignment 1 due
Computation Resources Overview
February 16
Computational Infrastructure (TA Lecture) 
Implementation: Neural Network Basics and Run-
through (TA Lecture)
Quick walk-through: Colaboratory, Google Cloud
Platform 
Introduction to PyTorch 
Handling data (images, text) 
Structuring a neural network and machine learning
codebase 
slides
colab (1)
colab (2)
Neural Network Basics - II
Febuary 18
No class
Inclement Weather Guidance
Snow day
Febuary 23
library_books
Training Neural Networks
Initialization
slides
(backprop)
Assignment 2 out
Febuary 25
Backpropagation
Optimization & hyperparameters
slides (opt)
March 2 
library_books
March 4
Training Caveats (Neural Networks and ML models)
Overfitting
Bias/Variance trade-off
Optimization & hyperparameters 
Improving Performance of Neural Networks
Optimization tips and tricks
Best principles
(see above)
Assignment 2 due
Convolutional Neural Networks (ConvNets)
March 9
Introduction to ConvNets
Convolutions
Pooling
slides
Exam:
March 11
library_books
Midterm Exam
In class
Assignmnt 3 out
March 16
March 18
Spring Break
March 23
March 25
library_books
ConvNet Architectures
Popular architectures (primarily images, brief overview
of videos)
Intuitions and key-insights
Design principles
(see above)
Assignment 3 due
Applications of ConvNets
March 30
library_books
April 1
April 6
Application I: Object Detection
slides
Project proposals
due
April 6
April 8
Application II: Dense Prediction
slides
Recurrent Neural Networks (RNNs)
April 13
April 15
library_books
Introduction to Recurrent Networks
RNNs, GRUs, LSTMs
Text/Language Applications
Language Modeling
slides
Assignment 4 due
April 15
April 20
library_books
Introduction to Self-attention or Transformers
Self-attenton or Transformers
slides
Assignment 5 out
Advanced Topics
April 15
April 20
Vision + Language (models, tasks, training)
(see above)
April 20
April 23
April 27
library_books
April 29
library_books
Image Generative Models
Auto regressive models
GANs, pix2pix, CycleGAN, etc.
VAEs
Teasers: Text Generation, Self-supervised Learning
slides
Assignment 5 due
Bonus Assignment
6 out
May 4
May 6
library_books
(A brief) Introduction to (Deep) Reinforcement Learning
slides
Bonus Assignment
6 due
May 11
Ethics and Bias; Epilogue
slides
Exam:
May 17
library_books
Final Exam
10:30am - 12:30pm (class link, live, virtual)
(
schedule
)
Syllabus
Prerequisites
Minimum grade of C- in CMSC330 and CMSC351; and 1 course with a minimum grade of C- from (MATH240, MATH461); 
and permission
of CMNS-Computer Science department.
We will work extensively with probability, statistics, mathematical functions such as logarithms and 
differentiation, and linear algebra
concepts such as vectors and matrices. You should be comfortable 
manipulating these concepts.
We will use of the Python programming language. It is assumed that you know or will quickly learn how to 
program in Python. The
programming assignments will be oriented toward Unix-like operating systems. While it 
is possible to complete the course using other
operating systems, you will be solely responsible for 
troubleshooting any issues you encounter.
If you are unsure that you have the required prerequisites, consult with the instructor.
Grading and collaboration
Here's how you will be graded:
Assignments: 35%
Project (presentation/poster and report): 35%
Exams (1 mid-term, 1 final): 30%
Bonus points for top-3 ranks for each challenge assignment
Collaboration: Students are expected to finish the homeworks by himself/herself, but discussion on the 
assignments is allowed (and
encouraged). The people you discussed with on assignments should be clearly 
detailed: before the solution to each question, list all people
that you discussed with on that particular 
question. In addition, each student should submit his/her own code and mention anyone he/she
collaborated 
with.
Submitting assignments and project reports, format, etc.
Details will be announced in class.
Notes
Syllabus subject to change.
Resources
The course should be self contained, but if you need additional reading material, you can consult the 
following:
Deep learning
, Goodfellow, Bengio and 
Courville, 2016 (book)
Recent Advances in Convolutional Neural 
Networks
, Gu et al., 2015 (review paper)
Helpful review and reference material:
Probability Review
, Arian Maleki and Tom Do
Linear Algebra Review
, Zico Kolter
If you need reference/additional readings for statistical learning, you can consult the following:
An Introduction to Statistical Learning
, 
James, Witten, Hastie, Tibshirani, 7th edition (book)
Pattern Recognition and Machine Learning
, Bishop, 2006 (book)
Useful interactive textbooks/courses available online
Machine Learning 
Crash Course
 -- an interactive online course by folks at Google
Dive into Deep Learning
 -- an interactive 
online book by folks at Amazon
Neural Networks and Deep Learning
 -- an online 
book by Michael Nielsen
Previous iterations of this course
CMSC498L: Introduction to Deep Learning 
(2019)
, Abhinav Shrivastava
CMSC498L: Introduction to Deep Learning 
(2020)
, Abhinav Shrivastava
Other deep learning courses
UT Austin CS 342: Neural networks
, Philipp Krähenbühl
UIUC CS 498L: Introduction to Deep Learning
, 
Svetlana Lazebnik
Stanford CS230: Deep Learning
, Andrew Ng and Kian Katanforoosh
Stanford CS231n: Convolutional Neural Networks for Visual 
Recognition
Princeton COS 495: Introduction to 
Deep Learning
, Yingyu Liang
IDIAP EE559: Deep Learning
, François Fleuret
ENS Deep Learning: Do It Yourself
, Marc Lelarge
Linear algebra material that'll help
Derivatives with respect 
to vectors
Matrix 
Differentiation
 by Randal J. Barnes
Vector, Matrix, and Tensor 
Derivatives
 by Erik Learned-Miller
The Matrix Calculus You Need For 
Deep Learning
 by Terence Parr and Jeremy Howard
Cheatsheet
Linear Algebra Review and Reference
 by Zico Kolter
CMU Graphics slides
 (slide 28 onwards)
Linear Algebra Review
 by 
Jian Xiang
General reference: 
Matrix Cookbook
 by Petersen & Pedersen (see derivatives)
Tutorials (libraries and computation resources)
PyTorch tutorial
TensorFlow tutorial
TA Neural 
Network Overview
Accommodations and Policies
Late Days
You get a total of 7 late days (to be used in 24-hour blocks) which can be used throughout the course.
Each late day is bound to only one submission. For example, if one assignment and one report are submitted 
3 hours after the
deadline, this results in 2 late days being used.
Late days cannot be used for the final project presentation, the final project report, 
the final 
assignment
, and any of the exams.
Once these late days are exhausted, any submissions turned in late will be penalized 25% per late day. 
Therefore, no submission will
be accepted more than three days after its due date. Except where the 
submission is not eligible to utilize late days (see above), in
which case the submission will be accepted 
after the deadline.
If you submit a submission multiple times, only the last one will be taken into account. If the last 
submission was after the deadline,
late days will be deducted accordingly.
If you are unsure about the late day policy, contact the instructor before utilizing them. See below for 
exceptions.
Academic Integrity
Note that academic dishonesty includes not only cheating, fabrication, and plagiarism, but also includes 
helping other students commit acts
of academic dishonesty by allowing them to obtain copies of your work. In 
short, all submitted work must be your own. Cases of academic
dishonesty will be pursued to the fullest extent 
possible as stipulated by the 
Office of Student Conduct
. It 
is very important for you to be
aware of the consequences of cheating, fabrication, facilitation, and 
plagiarism. For more information on the Code of Academic Integrity or
the Student Honor Council, please visit 
http://www.shc.umd.edu.
Excused Absences and Academic Accommodations
Any student who needs to be excused for an absence from a single lecture, recitation, or lab due to a 
medically necessitated absence shall:
Make a reasonable attempt to inform the instructor of his/her illness prior to the class.
Upon returning to the class, present their instructor with a self-signed note attesting to the date of 
their illness. Each note must
contain an acknowledgment by the student that the information provided is true 
and correct. Providing false information to
University officials is prohibited under Part 9(i) of the Code 
of Student Conduct (V-1.00(B) University of Maryland Code of Student
Conduct) and may result in disciplinary 
action.
This self-documentation may not be used for the Major Scheduled Grading Events as defined below.
Any student who needs to be excused for a Major Scheduled Grading Event, must provide written documentation 
of the illness from the
Health Center or from an outside health care provider. This documentation must verify 
dates of treatment and indicate the time frame that
the student was unable to meet academic responsibilities. 
No diagnostic information shall be given. The Major Scheduled Grading Events
for this course include midterm 
and final exam. For class presentations, the instructor will help the student swap their presentation slot
with other students.
It is also the student's responsibility to inform the instructor of any intended absences from exams and 
class presentations for religious
observances in advance. Notice should be provided as soon as possible, but 
no later than the Monday prior to the the midterm exam, the
class presentation date, and the final exam.
Any student eligible for and requesting reasonable academic accommodations due to a disability is requested 
to provide a letter of
accommodation from the Office of Disability Support Services within the first three 
weeks of the semester.
Other Accommodations and Policies
You can find the university’s course policies 
here
.

===================[Syllabus End]===================
Please examine the attached course syllabus carefully and provide detailed answers to the research questions (RQ) listed below. Each question focuses on specific aspects of "computing systems" tailored for AI/ML scalability. We are looking for specific issues and topics related to compilers, runtime systems, hardware acceleration, code optimization, programming model for AI/ML covered in the syllabus. Programming with Python or jupyter does not count as computing system topics.

RQ 1. Course Content and Frequency:
1.1 How frequently are topics explicitly related to "computing system" specialized for ML/AI discussed in the course? 
The topics are 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in likert scale: 
Frequent (4): At least one dedicated lecture discussed the topics.
Intermittent (3): The topics are discussed occasionally. 
Infrequent (2): The topics are rarely mentioned.
Never mentioned (1): The topics are never mentioned.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 2. Definition and Understanding:
2.1 How are the impacts of "computing systems" on AI/ML explicitly defined and explained in undergraduate curricula? 
The definition and explanation should include concepts of 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Adequate (3): Provide detailed definition and explanation.
Inadequate (2): Many of the topics missed significant discussion in lectures or in assignments.
Undefined (1): The topics are mostly undefined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
2.2 Do courses provide a comprehensive and explicit definition of impacts of "computing systems" on AI/ML?
The definition and explanation should include concepts such as 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer by providing the list of above topics (1 to 9) discussed in the course. Make it short and direct. Limit in 100 words. Do not include topics unrelated to "computing systems" like general ML/AI algorithms.

RQ 3. Requirement Specification:
3.1 How are computational performance and capability requirements for hardware and software systems running scalable AI/ML, explicitly specified and discussed in undergraduate courses?
Topics include 1) Computational Power (CPU, GPU, TPU, Edge AI chips), Memory and Storage, Network for scalable (parallel and distributed) model training, inference; 2) Distributed Computing Frameworks such as TensorFlow's Distributed Strategy, PyTorch's Distributed Data Parallel (DDP), and Horovod 3)  Optimization Techniques such as Efficient Algorithm, Quantization, Prunning 4) Programming Models and Abstractions such as High-Level Libraries (Tensorflow, PyTorch, Keras)
Answer in Likert scale: 
Quantitatively (3): The lectures or assignments provide numerical values for computational performance and capability requirements such as latency, throughput, resource utilization etc.
Qualitatively (2): The lectures used descriptive terms.
No guidelines (1): The Lecture provide no guidelines.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
3.2 How did the discussion of “computing system” requirements rank against the discussion of general AI/ML topics?
Answer in Likert scale: 
Equally discussed with other AI/ML topics (3)
“computing system” requirements is a sub topic (2) 
“computing system” requirements were never discussed (1) 
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 4. Influence and Importance:
4.1 How is the importance of various “computing system” factors of designing and maintaining scalable AI/ML emphasized in the course?
The factors are 1) scalable (parallel and distributed) model training and inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Holistic (2): The course took into account of many of the above factors.
System (1): The course viewed the factors as low level system issue, relegating responsibility to correct choice of hardware, programming model and AI/ML framework.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.

RQ 5. Case Studies and Real-World Applications:
5.1 Are real-world case studies involving hardware and software systems for AI/ML, with a focus on scalable model training, inference, and serving explicitly included in the curriculum?
Answer in Likert scale: 
Major (2): Computational performance and capability of the underlying system was the major concerns of the case studies.
Minor (1): Computational performance and capability of the underlying system was not a major concern of the case studies.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 6. Awareness and Integration of AI-Specific Engineering Practices:
6.1 Do the courses discuss contributions and best practices from AI/ML system engineering communities, specifically in areas such as compilers, runtime systems, hardware acceleration, and code optimization?
Answer in Likert scale: 
Adequate (3): The courses thoroughly cover contributions from AI/ML system engineering communities and best practices in detail by depicting from state of art.
Inadequate (2): The courses mention the topic but do not cover it in sufficient depth or detail.
Undefined (1): The coverage of this topic in the courses is unclear or not well-defined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 7. Projects and Practical Implementation:
7.1 To what extent do the assignments in the course provide hands-on experience with designing, building, and maintaining both scalable hardware and software systems for AI/ML, specifically focusing on compiler optimization, optimizing runtime systems, hardware acceleration, or code optimization for AI/ML?
Answer in Likert scale: 
Adequate (3): The assignments thoroughly cover these areas and provide extensive hands-on experience.
Inadequate (2): The assignments cover these areas minimally and do not provide sufficient hands-on experience.
None (1): The assignments do not cover these areas or provide relevant hands-on experience.
Could not be evaluated (0): Insufficient information or exposure to the assignments on the syllabus to provide an evaluation.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
