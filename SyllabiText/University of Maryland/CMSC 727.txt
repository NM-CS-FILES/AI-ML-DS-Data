==================[Syllabus Start]==================
 1 CMSC 727       Neural Computation      Spring 2022   Time and Place: Tuesdays and Thursdays, 12:30 pm – 1:45 pm, ONLINE Instructor:         James Reggia      myLastName AT umd DOT edu                     Office Hours:      Thursdays, 2 pm – 3 pm, or by appointment Teaching Asst.:  Hanyu Wang       hywang66 AT umd DOT edu                             Office Hours:      Mon. 11 am – noon, Weds. 2 pm – 3 pm, or by appointment Prerequisites: Graduate standing or permission of instructor. We assume that you have a basic background in undergraduate calculus, linear algebra, elementary probability theory, and Python programming.  No background in neurobiology/neuroscience is assumed. Class web page:  https://www.cs.umd.edu/class/spring2022/cmsc727/ Gives exam dates, homework assignments and their due dates, lecture slides, reading assignments, links to online class sessions and office hours, and links to other useful information. Content:  The field of neural computation involves the study of representations and algorithms that are inspired by information processing in the brain. This field has been growing very rapidly, reflecting its successful applications in machine learning and increasing interest in its potential to produce an artificial mind. In this course we will systematically cover the most important neural computation methods, illustrate their use for machine learning and for cognitive/brain modeling, provide hands-on experience training neural models, and introduce active research and application areas such as deep learning and programmable neural networks. The enormous amount of information currently available about neural networks means that we will be selective in what we cover this semester. In doing this we will emphasize the role of neural computation in machine learning and AI, although some multi-disciplinary information (cognitive science, neuroscience, etc.) will also be presented.  By the end of the semester you should be familiar with many of the central concepts and commonly used neural network methods as outlined below. 1. Basic concepts: how the brain computes; neural network architectures, activity dynamics and learning rules; self-organization and emergent intelligence; competition and cooperation in networks, local vs. distributed representations, historical perspective, applications. 2. Basic feedforward architectures using supervised learning: logical neurons, perceptrons, delta rule learning with linear associative memory, gradient descent, logistic regression, and error backpropagation. Some core concepts from machine learning (data standardization, performance assessment, overfitting, regularization, etc.) will be reviewed along the way. 3. Basic recurrent architectures using Hebbian learning: Hebb's rule, linear autoassociative memories, neural nets as dynamical systems, attractor networks (e.g., Hopfield nets, BSB), energy functions, simulated annealing, Boltzmann machines, restricted Boltzmann machines. 4. Deep learning: brain inspiration, Neocognitron, deep convolution networks, generative adversarial networks, deep belief networks, deep Boltzmann machines, deep autoencoders, machine vision & natural language processing applications. 5. Other feedforward methods: error backpropagation variations/extensions (e.g., RPROP), interpreting trained networks, radial basis function networks. 6. Contemporary architectures using supervised learning: echo state networks, recurrent backpropagation networks, backpropagation through time, long short-term memory (LSTM), attention mechanisms, transformer networks, hyperdimensional computing. 
 2 7. Unsupervised or no learning: competitive learning, self-organizing feature maps, adaptive resonance theory, continuous energy-minimizing attractor nets, oscillatory neural nets. 8. Reverse engineering the brain: neurobiological basis of cognition, large-scale brain models, brain-inspired neurocognitive architectures, programmable neural networks, neural virtual machines, NeuroLisp, strong AI and the artificial consciousness movement, prospects for a technological singularity. 9. Related topics as time permits: ensemble methods, neural nets in reinforcement learning, swarm intelligence and its relation to neural networks, evolutionary computation and neural networks, learning of activation rules, applications, software packages, hardware implementations and neuromorphic systems, large-scale networks, spiking neurons. Workload and Grading: There will be regular reading and homework assignments. Assignments will typically include some online work using Python and related code. Assignments and examinations are always to be treated as independent work. A more open-ended and substantial small group “semester project” is required. Grading will be based on homework assignments, quizzes, and class participation (collectively 20%), semester project and presentation (30%), midterm (25%), and second exam (25%).  Reading Sources:  Charu Aggarwal, Neural Networks and Deep Learning, Springer, 2018. ISBN: 978-3-319-94462-3 will serve as the primary textbook with regular assigned readings. R. O’Reilly, et al., Computational Cognitive Neuroscience, 4th Edition, 2020. A free pdf version of this book is available at: https://github.com/CompCogNeuro/ed4   I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016. A free pdf version of this book is available online at http://www.deeplearningbook.org  Additional readings from the literature will be posted as pdf files on the class web site.  Examinations: Exam 1: Thursday, March 17         Exam 2: Tuesday, May 10                             Final Exam Slot: Tuesday, May 17, 1:30 pm – 3:30 pm (reserve this time)  Disabilities: Any student eligible for and requesting reasonable academic accommodations due to a disability needs to provide the instructor with a letter of accommodation from the Office of Disability Support Services within the first two weeks of the semester. Class Absence Policy: The campus has a policy governing class absences.  This policy requires instructors to provide the following information. For CMSC 727, the “major scheduled grading events” are the two exams and the semester project.  A maximum of one self-signed medical excuse for late submission of other grading events will be accepted.  Academic Integrity: All homework assignments and examinations are to be done individually and independently unless specifically stated otherwise in writing; all submitted work must be your own. All students are expected to be familiar with and to uphold the Code of Academic Integrity (see https://studentconduct.umd.edu/you/students ). Posting homework solutions online violates academic integrity policy. Further details of CS Dept. Academic Integrity policies are at http://www.cs.umd.edu//class/resources/academicIntegrity.html  

===================[Syllabus End]===================
Please examine the attached course syllabus carefully and provide detailed answers to the research questions (RQ) listed below. Each question focuses on specific aspects of "computing systems" tailored for AI/ML scalability. We are looking for specific issues and topics related to compilers, runtime systems, hardware acceleration, code optimization, programming model for AI/ML covered in the syllabus. Programming with Python or jupyter does not count as computing system topics.

RQ 1. Course Content and Frequency:
1.1 How frequently are topics explicitly related to "computing system" specialized for ML/AI discussed in the course? 
The topics are 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in likert scale: 
Frequent (4): At least one dedicated lecture discussed the topics.
Intermittent (3): The topics are discussed occasionally. 
Infrequent (2): The topics are rarely mentioned.
Never mentioned (1): The topics are never mentioned.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 2. Definition and Understanding:
2.1 How are the impacts of "computing systems" on AI/ML explicitly defined and explained in undergraduate curricula? 
The definition and explanation should include concepts of 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Adequate (3): Provide detailed definition and explanation.
Inadequate (2): Many of the topics missed significant discussion in lectures or in assignments.
Undefined (1): The topics are mostly undefined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
2.2 Do courses provide a comprehensive and explicit definition of impacts of "computing systems" on AI/ML?
The definition and explanation should include concepts such as 1) scalable (parallel and distributed) model training, inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer by providing the list of above topics (1 to 9) discussed in the course. Make it short and direct. Limit in 100 words. Do not include topics unrelated to "computing systems" like general ML/AI algorithms.

RQ 3. Requirement Specification:
3.1 How are computational performance and capability requirements for hardware and software systems running scalable AI/ML, explicitly specified and discussed in undergraduate courses?
Topics include 1) Computational Power (CPU, GPU, TPU, Edge AI chips), Memory and Storage, Network for scalable (parallel and distributed) model training, inference; 2) Distributed Computing Frameworks such as TensorFlow's Distributed Strategy, PyTorch's Distributed Data Parallel (DDP), and Horovod 3)  Optimization Techniques such as Efficient Algorithm, Quantization, Prunning 4) Programming Models and Abstractions such as High-Level Libraries (Tensorflow, PyTorch, Keras)
Answer in Likert scale: 
Quantitatively (3): The lectures or assignments provide numerical values for computational performance and capability requirements such as latency, throughput, resource utilization etc.
Qualitatively (2): The lectures used descriptive terms.
No guidelines (1): The Lecture provide no guidelines.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
3.2 How did the discussion of “computing system” requirements rank against the discussion of general AI/ML topics?
Answer in Likert scale: 
Equally discussed with other AI/ML topics (3)
“computing system” requirements is a sub topic (2) 
“computing system” requirements were never discussed (1) 
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 4. Influence and Importance:
4.1 How is the importance of various “computing system” factors of designing and maintaining scalable AI/ML emphasized in the course?
The factors are 1) scalable (parallel and distributed) model training and inference; 2) testing, debugging, and monitoring of ML applications; 3) ML programming models and abstractions; 4) programming languages for machine learning; 5) ML compilers and runtimes; 6) specialized hardware for machine learning; 7) hardware-efficient ML methods; 8) machine learning benchmarks, and tooling.
Answer in Likert scale: 
Holistic (2): The course took into account of many of the above factors.
System (1): The course viewed the factors as low level system issue, relegating responsibility to correct choice of hardware, programming model and AI/ML framework.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.

RQ 5. Case Studies and Real-World Applications:
5.1 Are real-world case studies involving hardware and software systems for AI/ML, with a focus on scalable model training, inference, and serving explicitly included in the curriculum?
Answer in Likert scale: 
Major (2): Computational performance and capability of the underlying system was the major concerns of the case studies.
Minor (1): Computational performance and capability of the underlying system was not a major concern of the case studies.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 6. Awareness and Integration of AI-Specific Engineering Practices:
6.1 Do the courses discuss contributions and best practices from AI/ML system engineering communities, specifically in areas such as compilers, runtime systems, hardware acceleration, and code optimization?
Answer in Likert scale: 
Adequate (3): The courses thoroughly cover contributions from AI/ML system engineering communities and best practices in detail by depicting from state of art.
Inadequate (2): The courses mention the topic but do not cover it in sufficient depth or detail.
Undefined (1): The coverage of this topic in the courses is unclear or not well-defined.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
RQ 7. Projects and Practical Implementation:
7.1 To what extent do the assignments in the course provide hands-on experience with designing, building, and maintaining both scalable hardware and software systems for AI/ML, specifically focusing on compiler optimization, optimizing runtime systems, hardware acceleration, or code optimization for AI/ML?
Answer in Likert scale: 
Adequate (3): The assignments thoroughly cover these areas and provide extensive hands-on experience.
Inadequate (2): The assignments cover these areas minimally and do not provide sufficient hands-on experience.
None (1): The assignments do not cover these areas or provide relevant hands-on experience.
Could not be evaluated (0): Insufficient information or exposure to the assignments on the syllabus to provide an evaluation.
Provide the score based on overall discussion of the above topics. Do not rate each topic individually. No explanation needed.
